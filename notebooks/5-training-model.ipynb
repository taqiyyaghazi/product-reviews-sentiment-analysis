{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(category):\n",
    "    train = pd.read_csv('../data/processed/train/' + category + '_train.csv')\n",
    "    test = pd.read_csv('../data/processed/test/' + category + '_test.csv')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_train, food_test = load_data('food')\n",
    "electronic_train, electronic_test = load_data('electronic')\n",
    "fashion_train, fashion_test = load_data('fashion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviews    99\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rasa biasa harga mahal kualitas standar kira b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harga murah barang cuma kirim rada lama thank</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rasa coklat kualitas harga jangkau emas baik k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harga baru beli susu kambing bubuk merk kualit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paket terima selamat cacat cepet emas kirim seler</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>mantap bonus alhamdulilah bumbu rembes aman pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>kualitas produk baik harga produk baik grentea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>rasa kayak enak harga harga mahal harga</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8335</th>\n",
       "      <td>harga baik kualitas baik rasa baik terimakasih...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8336</th>\n",
       "      <td>harga mahal kualitas buruk rasa enak sama seka...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8238 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  label\n",
       "0     rasa biasa harga mahal kualitas standar kira b...      0\n",
       "1         harga murah barang cuma kirim rada lama thank      1\n",
       "2     rasa coklat kualitas harga jangkau emas baik k...      1\n",
       "3     harga baru beli susu kambing bubuk merk kualit...      0\n",
       "4     paket terima selamat cacat cepet emas kirim seler      1\n",
       "...                                                 ...    ...\n",
       "8332  mantap bonus alhamdulilah bumbu rembes aman pl...      1\n",
       "8333  kualitas produk baik harga produk baik grentea...      1\n",
       "8334            rasa kayak enak harga harga mahal harga      0\n",
       "8335  harga baik kualitas baik rasa baik terimakasih...      1\n",
       "8336  harga mahal kualitas buruk rasa enak sama seka...      0\n",
       "\n",
       "[8238 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_train = food_train.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Food Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = food_train['reviews'], food_train['label']\n",
    "X_test, y_test = food_test['reviews'], food_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51      NaN\n",
       "222     NaN\n",
       "325     NaN\n",
       "424     NaN\n",
       "521     NaN\n",
       "       ... \n",
       "7872    NaN\n",
       "7879    NaN\n",
       "8047    NaN\n",
       "8091    NaN\n",
       "8152    NaN\n",
       "Name: reviews, Length: 99, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Project\\product-reviews-sentiment-analysis\\notebooks\\5-training-model.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Project/product-reviews-sentiment-analysis/notebooks/5-training-model.ipynb#ch0000009?line=0'>1</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m), min_df\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Project/product-reviews-sentiment-analysis/notebooks/5-training-model.ipynb#ch0000009?line=1'>2</a>\u001b[0m features \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X_train)\n",
      "File \u001b[1;32me:\\Project\\product-reviews-sentiment-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2057'>2058</a>\u001b[0m \u001b[39m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2058'>2059</a>\u001b[0m \n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2059'>2060</a>\u001b[0m \u001b[39mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2073'>2074</a>\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2074'>2075</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2075'>2076</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m-> <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2076'>2077</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2077'>2078</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2078'>2079</a>\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=2079'>2080</a>\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32me:\\Project\\product-reviews-sentiment-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32me:\\Project\\product-reviews-sentiment-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1198'>1199</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1199'>1200</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32me:\\Project\\product-reviews-sentiment-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:108\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=85'>86</a>\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=86'>87</a>\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=87'>88</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=103'>104</a>\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=104'>105</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=106'>107</a>\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=107'>108</a>\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=108'>109</a>\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=109'>110</a>\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32me:\\Project\\product-reviews-sentiment-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:226\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=222'>223</a>\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=224'>225</a>\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[1;32m--> <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=225'>226</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=226'>227</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=227'>228</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///e%3A/Project/product-reviews-sentiment-analysis/.venv/lib/site-packages/sklearn/feature_extraction/text.py?line=229'>230</a>\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "features = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingData(X, y):\n",
    "    #membangun vector space model/pembobotan dengan tfidf\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "    features = vectorizer.fit_transform(X)\n",
    "\n",
    "    #melakukan split data training untuk mengetahui akurasi\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.1, random_state=4)\n",
    "\n",
    "    #modeling sentiment\n",
    "    LR_ = LogisticRegression(C=3, solver='liblinear', max_iter=150).fit(X_train, y_train)\n",
    "\n",
    "    # melakukan evaluasi\n",
    "    yhat = LR_.predict(X_test)\n",
    "\n",
    "    yhat_prob = LR_.predict_proba(X_test)\n",
    "    \n",
    "    return LR_, vectorizer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c11b6e3dd3c40971b4e52db259878f5dc8d878adefe17888dc47bc66d82c736b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
